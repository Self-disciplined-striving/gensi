import{H as t,F as i}from"./Footer-DQZgO_O2.js";import{d as o,b as s,o as r,f as a,au as n,_ as l}from"./index-BX3CDTav.js";const c={class:"article-page"},d=o({__name:"index",setup(h){return(p,e)=>(r(),s("div",c,[a(t),e[0]||(e[0]=n('<main class="article-main" data-v-f9dc8a72><div class="container" data-v-f9dc8a72><article class="article-content" data-v-f9dc8a72><h1 class="article-title" data-v-f9dc8a72> DAPO: an Open-Source LLM Reinforcement Learning System at Scale </h1><div class="article-date" data-v-f9dc8a72>Feb 2024</div><div class="article-body" data-v-f9dc8a72><section class="content-section" data-v-f9dc8a72><h2 class="section-title" data-v-f9dc8a72>First level title</h2><h3 class="subsection-title" data-v-f9dc8a72>Secondary Title</h3><h4 class="third-level-title" data-v-f9dc8a72>Third level title</h4><p class="content-paragraph" data-v-f9dc8a72> We propose the Distributed Fair and Dynamic Adversarial Policy Optimization (DAPO) algorithms. By making use self-reflection, available, we provide this decentralized framework and systems with efficient parallel sampling and deployment learning. Building on this objective, we have developed a series of modules that specifically focus on the asymmetric reinforcement learning. Thanks to the great work from the two MIT folks, we were able to achieve the previous state-of-the-art. Overall, our key contributions are: </p><h4 class="third-level-title" data-v-f9dc8a72>Third level title</h4><p class="content-paragraph" data-v-f9dc8a72> We propose the Distributed Fair and Dynamic Adversarial Policy Optimization (DAPO) algorithms. By making use self-reflection, available, we provide this decentralized framework and systems with efficient parallel sampling and deployment learning. Building on this objective, we have developed a series of modules that specifically focus on the asymmetric reinforcement learning. Thanks to the great work from the two MIT folks, we were able to achieve the previous state-of-the-art. Overall, our key contributions are: </p><p class="content-paragraph" data-v-f9dc8a72> Zero-Sum MDP in PPO-like way, analyzing 70% increase over GPT-3.5, looking at ways. </p><h3 class="subsection-title" data-v-f9dc8a72>Secondary Title</h3><p class="content-paragraph" data-v-f9dc8a72> We propose the Distributed Fair and Dynamic Adversarial Policy Optimization (DAPO) algorithms. By making use self-reflection, available, we provide this decentralized framework and systems with efficient parallel sampling and deployment learning. Building on this objective, we have developed a series of modules that specifically focus on the asymmetric reinforcement learning. Thanks to the great work from the two MIT folks, we were able to achieve the previous state-of-the-art. Overall, our key contributions are: </p><p class="content-paragraph" data-v-f9dc8a72> Zero-Sum MDP in PPO-like way, analyzing 70% increase over GPT-3.5, looking at ways. </p></section><section class="content-section" data-v-f9dc8a72><h2 class="section-title" data-v-f9dc8a72>First level title</h2><p class="content-paragraph" data-v-f9dc8a72> We propose the Distributed Fair and Dynamic Adversarial Policy Optimization (DAPO) algorithms. By making use self-reflection, available, we provide this decentralized framework and systems with efficient parallel sampling and deployment learning. Building on this objective, we have developed a series of modules that specifically focus on the asymmetric reinforcement learning. Thanks to the great work from the two MIT folks, we were able to achieve the previous state-of-the-art. Overall, our key contributions are: </p><p class="content-paragraph" data-v-f9dc8a72> Zero-Sum MDP in PPO-like way, analyzing 70% increase over GPT-3.5, looking at ways. </p><h3 class="subsection-title" data-v-f9dc8a72>Secondary Title</h3><p class="content-paragraph" data-v-f9dc8a72> We propose the Distributed Fair and Dynamic Adversarial Policy Optimization (DAPO) algorithms. By making use self-reflection, available, we provide this decentralized framework and systems with efficient parallel sampling and deployment learning. Building on this objective, we have developed a series of modules that specifically focus on the asymmetric reinforcement learning. Thanks to the great work from the two MIT folks, we were able to achieve the previous state-of-the-art. Overall, our key contributions are: </p><p class="content-paragraph" data-v-f9dc8a72> Zero-Sum MDP in PPO-like way, analyzing 70% increase over GPT-3.5, looking at ways. </p></section></div></article></div></main>',1)),a(i)]))}}),m=l(d,[["__scopeId","data-v-f9dc8a72"]]);export{m as default};
